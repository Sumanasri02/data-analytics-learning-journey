# Model Evaluation and Tuning

## Why Model Evaluation Matters
Model evaluation helps measure how well a machine learning
model generalizes to unseen data.

A good model is not the one that fits training data well,
but the one that performs well on new data.

---

## Evaluation Metrics

### Regression
- Mean Squared Error (MSE)
- Root Mean Squared Error (RMSE)
- R² Score

### Classification
- Accuracy
- Precision
- Recall
- F1-score
- ROC-AUC

---

## Cross Validation

Cross-validation helps:
- Reduce overfitting
- Use data efficiently
- Get reliable performance estimates

Common types:
- K-Fold Cross Validation
- Stratified K-Fold

---

## Hyperparameter Tuning

Hyperparameters are set before training the model.

Common techniques:
- Grid Search
- Random Search

---

## Bias-Variance Tradeoff

- High Bias → Underfitting
- High Variance → Overfitting

---

## Tools Used

- Scikit-learn
- NumPy
- Pandas

---

## Key Takeaways

- Always evaluate before deploying
- Cross-validation improves reliability
- Tuning boosts performance
# Model Evaluation Metrics

## Regression Metrics
## Classification Metrics
## When to Use Which Metric

# Cross Validation

## Why Cross Validation
## K-Fold Cross Validation
## Stratified K-Fold
# Hyperparameter Tuning

## What are Hyperparameters
## Grid Search
## Random Search

